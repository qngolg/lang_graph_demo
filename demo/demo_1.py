import os
from typing import Annotated, TypedDict, List, Optional
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
import asyncio

from dotenv import load_dotenv

load_dotenv()
# --------------------------
# 1. å®šä¹‰çŠ¶æ€ç±»å‹
# --------------------------
class GraphState(TypedDict):
    messages: List[HumanMessage | AIMessage]
    current_step: str
    partial_response: str  # ç”¨äºç´¯ç§¯æµå¼ç”Ÿæˆçš„ç‰‡æ®µ
    is_complete: bool

# --------------------------
# 2. åˆå§‹åŒ–å¤§æ¨¡å‹ï¼ˆæ”¯æŒæµå¼ï¼‰
# --------------------------

api_key = os.getenv("aliyun_sk")
base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
model = "gpt-3.5-turbo"

llm = ChatOpenAI(
    model="qwen-plus",
    api_key=api_key,
    base_url=base_url,
    temperature=0.7)

# --------------------------
# 3. å®šä¹‰èŠ‚ç‚¹ï¼šè°ƒç”¨å¤§æ¨¡å‹å¹¶æµå¼ç”Ÿæˆ
# --------------------------
async def generate_answer_streaming(state: GraphState) -> GraphState:
    print("\nğŸ¤– [èŠ‚ç‚¹] å¼€å§‹æµå¼ç”Ÿæˆç­”æ¡ˆ...")
    state["current_step"] = "generating"
    state["partial_response"] = ""

    # æ„é€ è¾“å…¥æ¶ˆæ¯
    user_msg = next((m.content for m in state["messages"] if isinstance(m, HumanMessage)), "")
    ai_message = HumanMessage(content=user_msg)  # ç®€åŒ–ï¼Œå®é™…åº”è¯¥æ„é€ åˆé€‚çš„ Message åˆ—è¡¨
    messages = [HumanMessage(content=user_msg)]  # ä½ å¯ä»¥æŒ‰éœ€æ„é€  prompt

    # è°ƒç”¨æµå¼å¤§æ¨¡å‹
    full_response = ""
    stream = llm.stream(messages)  # è¿”å›çš„æ˜¯ AIMessageChunk çš„ç”Ÿæˆå™¨

    print("ğŸ“¤ å¼€å§‹æµå¼è¾“å‡ºï¼ˆé€æ­¥æ‰“å°ï¼‰:")
    async for chunk in stream:
        content = chunk.content
        if content:
            print(content, end="", flush=True)  # å®æ—¶æ‰“å°æ¯ä¸ª token
            full_response += content
            state["partial_response"] += content  # ç´¯ç§¯åˆ°çŠ¶æ€ä¸­ï¼Œå¯è¢«åç»­èŠ‚ç‚¹ä½¿ç”¨

    print("\nâœ… [èŠ‚ç‚¹] æµå¼ç”Ÿæˆå®Œæˆ")
    state["messages"].append(AIMessage(content=full_response))  # æŠŠå®Œæ•´å›å¤åŠ å…¥æ¶ˆæ¯å†å²
    state["is_complete"] = True  # è¡¨ç¤ºæ­¤æ­¥éª¤å®Œæˆï¼Œå›¾å¯ä»¥è¿›å…¥ä¸‹ä¸€æ­¥
    return state

# --------------------------
# 4. å®šä¹‰å…¶ä»–èŠ‚ç‚¹ï¼ˆå¦‚å¼€å§‹ã€ç»“æŸç­‰ï¼‰
# --------------------------
def start(state: GraphState) -> GraphState:
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå›¾")
    return {
        "messages": [HumanMessage(content="è¯·ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„åŸç†ã€‚")],
        "current_step": "start",
        "partial_response": "",
        "is_complete": False
    }

def end(state: GraphState) -> GraphState:
    print("\nğŸ¯ å›¾æ‰§è¡Œå®Œæ¯•ï¼Œæœ€ç»ˆå›å¤:")
    final_msg = state["messages"][-1].content if state["messages"] else "æ— å›å¤"
    print(final_msg)
    return {
        "messages": state["messages"],
        "current_step": "end",
        "partial_response": state["partial_response"],
        "is_complete": True
    }

# --------------------------
# 5. æ„å»ºå›¾
# --------------------------
workflow = StateGraph(GraphState)

workflow.add_node("start", start)
workflow.add_node("generate", generate_answer_streaming)  # æµå¼èŠ‚ç‚¹
workflow.add_node("end", end)

workflow.set_entry_point("start")
workflow.add_edge("start", "generate")
workflow.add_edge("generate", "end")

app = workflow.compile()

# --------------------------
# 6. è¿è¡Œå›¾
# --------------------------
asyncio.run(app.invoke({}))